{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dmpa_6th_sem.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWemH6XxUFEp",
        "colab_type": "text"
      },
      "source": [
        "###Import all the necessary modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC6QlOfdUEx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os.path import isfile\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNOFy1fFUuJp",
        "colab_type": "text"
      },
      "source": [
        "###Choose Translate direction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pP2JBLp9Uznq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Set htoe as true for translation from Hindi to Hnglish; \n",
        "#Set htoe as false for English to Hindi translation\n",
        "htoe = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-4tRIHdUNN8",
        "colab_type": "text"
      },
      "source": [
        "###Load the Tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSidzxZZOC_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if htoe:\n",
        "    tokenizer_one = pickle.load(open(\"tokenizer_hi\",\"rb\"))\n",
        "    tokenizer_two = pickle.load(open(\"tokenizer_en\",\"rb\"))\n",
        "else:\n",
        "    pickle.load(open(\"tokenizer_en\",\"rb\"))\n",
        "    pickle.load(open(\"tokenizer_hi\",\"rb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0NwdDczUrlk",
        "colab_type": "text"
      },
      "source": [
        "###Create the dataset - **For training purposes only**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6BcRWYhUtHK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tweak the dataset params according to your needs\n",
        "\n",
        "MAX_LENGTH = 64 \n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8vodrv6VO9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the dataset objects\n",
        "\n",
        "if htoe:\n",
        "    raw_data_one = pickle.load(open(\"raw_data_hi\",\"rb\"))\n",
        "    raw_data_two = pickle.load(open(\"raw_data_en\",\"rb\"))\n",
        "else:\n",
        "    pickle.load(open(\"raw_data_en\",\"rb\"))\n",
        "    pickle.load(open(\"raw_data_hi\",\"rb\"))\n",
        "\n",
        "print(\"Dataset size: {}\".format(len(raw_data_one)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5JxTae_VPmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions for data preprocessing\n",
        "\n",
        "def encode(lang1, lang2):\n",
        "  lang1 = [len(tokenizer_one.word_index)] + tokenizer_one.texts_to_sequences(\n",
        "      [lang1.numpy().decode(\"utf-8\")])[0] + [len(tokenizer_one.word_index)+1]\n",
        "\n",
        "  lang2 = [len(tokenizer_two.word_index)] + tokenizer_two.texts_to_sequences(\n",
        "      [lang2.numpy().decode(\"utf-8\")])[0] + [len(tokenizer_two.word_index)+1]\n",
        "  \n",
        "  return lang1, lang2\n",
        "\n",
        "def tf_encode(lang1, lang2):\n",
        "  result_one, result_two = tf.py_function(\n",
        "      encode, [lang1, lang2], [tf.int64, tf.int64])\n",
        "  result_one.set_shape([None])\n",
        "  result_two.set_shape([None])\n",
        "\n",
        "  return result_one, result_two\n",
        "\n",
        "def filter_max_length(x, y, max_length=MAX_LENGTH):\n",
        "  return tf.logical_and(tf.size(x) <= max_length,\n",
        "                        tf.size(y) <= max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AMOBaiHVXti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Obtain tf-data object from the dataset\n",
        "\n",
        "train_examples = tf.data.Dataset.from_tensor_slices((raw_data_hi, raw_data_en)) \n",
        "\n",
        "train_preprocessed = (\n",
        "    train_examples \n",
        "    .map(tf_encode)\n",
        "    .filter(filter_max_length)\n",
        "    .cache()\n",
        "    .shuffle(BUFFER_SIZE))\n",
        "\n",
        "train_dataset = (train_preprocessed\n",
        "                 .padded_batch(BATCH_SIZE)\n",
        "                 .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6IkKAYnVUrb",
        "colab_type": "text"
      },
      "source": [
        "###Create the word embedding matrices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3iTs5X1VaS8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the vocab sizes\n",
        "\n",
        "if htoe:\n",
        "    input_vocab_size = len(tokenizer_hi.word_index) + 2\n",
        "    target_vocab_size = len(tokenizer_en.word_index) + 2\n",
        "else:\n",
        "    input_vocab_size = len(tokenizer_en.word_index) + 2\n",
        "    target_vocab_size = len(tokenizer_hi.word_index) + 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdImo-P8VdNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the pretrained embeddings\n",
        "\n",
        "words_en, embeddings_en = pickle.load(\n",
        "    open('polyglot-en.pkl', 'rb'), encoding='latin1')\n",
        "words_hi, embeddings_hi = pickle.load(\n",
        "    open('polyglot-hi.pkl', 'rb'), encoding='latin1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5OqV9X-Vfb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# English embedding matrix\n",
        "\n",
        "embeddings_index_en = {}\n",
        "word_index_en = tokenizer_en.word_index\n",
        "\n",
        "for i in range(len(words_en)):\n",
        "    embeddings_index_en[words_en[i].lower()] = embeddings_en[i]\n",
        "\n",
        "embedding_matrix_en = np.zeros((target_vocab_size, 64))\n",
        "for word, i in word_index_en.items():\n",
        "    embedding_vector = embeddings_index_en.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_en[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrYgRR3LViqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hindi embedding matrix\n",
        "\n",
        "embeddings_index_hi = {}\n",
        "word_index_hi = tokenizer_hi.word_index\n",
        "\n",
        "for i in range(len(words_hi)):\n",
        "    embeddings_index_hi[words_hi[i]] = embeddings_hi[i]\n",
        "\n",
        "embedding_matrix_hi = np.zeros((input_vocab_size, 64))\n",
        "for word, i in word_index_hi.items():\n",
        "    embedding_vector = embeddings_index_hi.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix_hi[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBApmP9bYiL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if htoe:\n",
        "    embedding_matrix_one = embedding_matrix_hi\n",
        "    embedding_matrix_two = embedding_matrix_en\n",
        "else:\n",
        "    embedding_matrix_one = embedding_matrix_en\n",
        "    embedding_matrix_two = embedding_matrix_hi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjKd29iJY52Y",
        "colab_type": "text"
      },
      "source": [
        "###Create the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sn7ARpFxY049",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the model Hyperparams\n",
        "\n",
        "num_layers = 6\n",
        "d_model = 64\n",
        "dff = 2048\n",
        "num_heads = 32\n",
        "dropout_rate = 0.1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlkn6ckKY74t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions for the model\n",
        "\n",
        "def check_files():\n",
        "    if htoe:\n",
        "        if(!isfile('transformer_htoe.weights.data-00000-of-00002') or\n",
        "           !isfile('transformer_htoe.weights.data-00001-of-00002') or\n",
        "           !isfile('transformer_htoe.weights.index')):\n",
        "\n",
        "           !unzip -o transformer_htoe_weights.zip\n",
        "    else:\n",
        "        if(!isfile('transformer_etoh.weights.data-00000-of-00002') or\n",
        "           !isfile('transformer_etoh.weights.data-00001-of-00002') or\n",
        "           !isfile('transformer_etoh.weights.index')):\n",
        "\n",
        "           !unzip -o transformer_etoh_weights.zip\n",
        "\n",
        "\n",
        "def positional_encoding(max_pos, d_model):\n",
        "    \"\"\" Returns the positional encoding for all positions\n",
        "\n",
        "    Args:\n",
        "        max_pos: (int) size of required positional embeddings equal to \n",
        "        the vocab size\n",
        "        d_model: (int) model size equal to the embedding size\n",
        "    \n",
        "    Returns:\n",
        "        pe: (tensor of type float32, shape = \n",
        "        (1, max_pos, d_model)) positional encodings of type float32\n",
        "    \"\"\"\n",
        "\n",
        "    theta = np.expand_dims(np.arange(max_pos), 1) / (np.power(10000, \n",
        "    (2 * np.expand_dims(np.arange(d_model), 0) // 2) / d_model))\n",
        "  \n",
        "    # sin(i) for all even i\n",
        "    theta[:, 0::2] = np.sin(theta[:, 0::2]) \n",
        "    \n",
        "    # cos(i) for all odd i\n",
        "    theta[:, 1::2] = np.cos(theta[:, 1::2]) \n",
        "    \n",
        "    pe = np.expand_dims(theta, 0)\n",
        "    \n",
        "    return tf.cast(pe, dtype=tf.float32)\n",
        "\n",
        "def create_padding_mask(seq):\n",
        "    \"\"\" Creates a padding mask for the self attention layer in the decoder\n",
        "    \"\"\"\n",
        "\n",
        "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "    \n",
        "    # (batch_size, 1, 1, seq_len)\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  \n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  \n",
        "  # (seq_len, seq_len)\n",
        "  return mask  \n",
        "\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  # (..., seq_len_q, seq_len_k)\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  \n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  # (..., seq_len_q, seq_len_k)\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  \n",
        "\n",
        "  # (..., seq_len_q, depth_v)\n",
        "  output = tf.matmul(attention_weights, v)  \n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "def print_out(q, k, v):\n",
        "  temp_out, temp_attn = scaled_dot_product_attention(\n",
        "      q, k, v, None)\n",
        "  print ('Attention weights are:')\n",
        "  print (temp_attn)\n",
        "  print ('Output is:')\n",
        "  print (temp_out)\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "\n",
        "    # (batch_size, seq_len, dff)\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])\n",
        "\n",
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-1mW4d_Y-Cu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the Model layers\n",
        "\n",
        "# Multihead Attention keras layer\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is :\n",
        "    (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    # (batch_size, seq_len, d_model)\n",
        "    q = self.wq(q)  \n",
        "    k = self.wk(k)\n",
        "    v = self.wv(v)\n",
        "    \n",
        "    # (batch_size, num_heads, seq_len_q, depth)\n",
        "    q = self.split_heads(q, batch_size)\n",
        "\n",
        "    # (batch_size, num_heads, seq_len_k, depth)  \n",
        "    k = self.split_heads(k, batch_size)\n",
        "\n",
        "    # (batch_size, num_heads, seq_len_v, depth)\n",
        "    v = self.split_heads(v, batch_size)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    # (batch_size, seq_len_q, num_heads, depth)\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  \n",
        "\n",
        "    # (batch_size, seq_len_q, d_model)\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  \n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights\n",
        "\n",
        "# Encoder keras Layer\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  \n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    out1 = self.layernorm1(x + attn_output)  \n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.ffn(out1)  \n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  \n",
        "    \n",
        "    return out2\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "        input_vocab_size, \n",
        "        d_model, \n",
        "        weights = [embedding_matrix_hi], \n",
        "        trainable = False)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                            self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    x = self.embedding(x)  \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    return x  \n",
        "\n",
        "# Decoder keras layer\n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  \n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  \n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    out2 = self.layernorm2(attn2 + out1)  \n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.ffn(out2)  \n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  \n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size,\n",
        "               maximum_position_encoding, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(\n",
        "        target_vocab_size, \n",
        "        d_model, \n",
        "        weights = [embedding_matrix_en], \n",
        "        trainable=False)\n",
        "    self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    # (batch_size, target_seq_len, d_model)\n",
        "    x = self.embedding(x) \n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKNtu-PfY_4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the Model\n",
        "\n",
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, pe_input, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, pe_target, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    # (batch_size, inp_seq_len, d_model)\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  \n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    final_output = self.final_layer(dec_output)  \n",
        "    \n",
        "    return final_output, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWXpDKiJZB5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the Learning Rate\n",
        "\n",
        "# Custom Learning rate scheduler\n",
        "\n",
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "\n",
        "# create the optimizer object with the custom learning rate\n",
        "\n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate, \n",
        "    beta_1=0.9, \n",
        "    beta_2=0.98, \n",
        "    epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAYh1J_hZEqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the loss for the model\n",
        "\n",
        "# Create the required type of loss\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, \n",
        "    reduction='none')\n",
        "\n",
        "# Define how loss is calculated\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "# Create the loss and accuracy objects\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(\n",
        "    name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGR2w5ALZGlm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the model object with the necessary params\n",
        "\n",
        "# Unzip the weights\n",
        "\n",
        "if !checkfiles():\n",
        "    !unzip -o transformer_htoe_weights\n",
        "    !unzip -o transformer_etoh_weights\n",
        "\n",
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, \n",
        "                          pe_input=input_vocab_size, \n",
        "                          pe_target=target_vocab_size,\n",
        "                          rate=dropout_rate)\n",
        "\n",
        "# Load any pre-trained weights (Comment out if retraining from scratch)\n",
        "\n",
        "if htoe:\n",
        "    transformer.load_weights('transformer_htoe.weights')\n",
        "else:\n",
        "    transformer.load_weights('transformer_etoh.weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dk56C0B1j4rI",
        "colab_type": "text"
      },
      "source": [
        "###Train the model - Not needed for predictions only"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mWqD9CAj2kL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define training epochs\n",
        "\n",
        "EPOCHS = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeIvLVWGj8RT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the training step function\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, _ = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    loss = loss_function(tar_real, predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxVJAb4dkA0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start training the model\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  for (batch, (inp, tar)) in enumerate(train_dataset):\n",
        "    train_step(inp, tar)\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "    \n",
        "  transformer.save_weights('transformer_{}.weights'.format(epoch + 1))\n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbZ1h08-kTuL",
        "colab_type": "text"
      },
      "source": [
        "###Predict sentences using the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WAUMu_XkUFf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility functions for prediction\n",
        "\n",
        "MAX_LENGTH = 64 \n",
        "\n",
        "def preprocess_string(s):\n",
        "    ''' String preprocessing function\n",
        "\n",
        "    Args:\n",
        "        s: The string to be preprocessed\n",
        "    \n",
        "    Returns:\n",
        "        s: The preprocessed String\n",
        "    '''\n",
        "\n",
        "    if htoe:\n",
        "        s = re.sub(r'[a-zA-Z]', '', s) # Removes english chars from hindi text\n",
        "    s = re.sub(r\"[\\(\\[].*?[\\)\\]]\", \"\", s) # Removes text between braces\n",
        "    s = re.sub(r'([!.?।])', r' \\1', s) #Includes space between some characters\n",
        "    s = re.sub(r'\\s+', r' ', s) #Reduces multispace string to a single space\n",
        "    \n",
        "    return s\n",
        "\n",
        "def evaluate(inp_sentence):\n",
        "  start_token = [len(tokenizer_one.word_index)]\n",
        "  end_token = [len(tokenizer_one.word_index) + 1]\n",
        "  \n",
        "  inp_sentence = start_token + tokenizer_one.texts_to_sequences(\n",
        "      [inp_sentence])[0] + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  decoder_input = [len(tokenizer_two.word_index)]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(64):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    \n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == len(tokenizer_two.word_index)+1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights\n",
        "\n",
        "def translate(sentence):\n",
        "  result, attention_weights = evaluate(sentence)\n",
        "  predicted_sentence = tokenizer_two.sequences_to_texts(\n",
        "      [[i.numpy()] for i in result if i < len(tokenizer_two.word_index)])  \n",
        "\n",
        "  return ' '.join(predicted_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9h-euuwlTUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Enter input sentence\n",
        "inp_str = \"क्या मैं आपकी मदद कर सकता हुँ ?\"\n",
        "\n",
        "print(translate(preprocess_string(inp_str)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}